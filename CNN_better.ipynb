{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cdb29b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset-master/JPEGImages/BloodImage_00280.jpg is in the csv, but does not exist\n",
      "./data/dataset-master/JPEGImages/BloodImage_00116.jpg is in the csv, but does not exist\n"
     ]
    }
   ],
   "source": [
    "##### IMPORTS ######\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "##### ARCHITECTURE #####\n",
    "class BloodCell(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # NETWORK\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            # 1\n",
    "            nn.Conv2d(3, 16, kernel_size = 3 , stride = (2,2)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.MaxPool2d(kernel_size =2),\n",
    "            \n",
    "            # 2\n",
    "            nn.Conv2d(16, 8, kernel_size = 3 , stride = (2,2)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.MaxPool2d(kernel_size =2),\n",
    "            \n",
    "            # 3\n",
    "            nn.Conv2d(8, 4, kernel_size = 3, stride = (2,2)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.MaxPool2d(kernel_size =2),\n",
    "        )\n",
    "\n",
    "        # AVGPOOL\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((15,15))\n",
    "\n",
    "        # CLASSIFIER\n",
    "        self.classifier = nn.Sequential(\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4*15*15,200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50,4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        xb = self.network(xb)\n",
    "        xb = self.avgpool(xb)\n",
    "        xb = self.classifier(xb)\n",
    "        return xb\n",
    "\n",
    "##### FUNCTIONS ######\n",
    "def load_csv(path) :\n",
    "    # 1 : read the csv\n",
    "    df = pd.read_csv(path+\"dataset-master/labels_full.csv\",sep =\",\")\n",
    "    \n",
    "    # 2 : remove images with missing labels\n",
    "    df = df[df['Category'].notnull()]\n",
    "    \n",
    "    # 3 : translate the \"Image\" column to the path of the actual image\n",
    "    df['Image'] = df['Image'].apply(lambda x : \n",
    "                                    \"BloodImage_\" \n",
    "                                    + (5-len(str(x)))*\"0\"\n",
    "                                    + str(x)\n",
    "                                    + \".jpg\")\n",
    "    # 4 drop unnecessary columns\n",
    "    df = df[['Image', 'Category']]\n",
    "    \n",
    "    # 5 drop categories without enough data to train the model on\n",
    "    category_value_count = df.Category.value_counts()\n",
    "    rare_categories = category_value_count[category_value_count < 10].index\n",
    "    df = df.loc[~df[\"Category\"].isin(rare_categories)]\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_folder(path, folder ,df) :\n",
    "    # reset and make folder\n",
    "    if(os.path.isdir(path+folder)) :\n",
    "        shutil.rmtree(path+folder) \n",
    "    os.mkdir(path+folder)\n",
    "    \n",
    "    # make subfolders\n",
    "    for categories in df.Category.unique():\n",
    "        if(not os.path.isdir(path+folder+\"/\"+categories)) :\n",
    "            os.mkdir(path+folder+\"/\"+categories)\n",
    "    \n",
    "    # fill subfolders\n",
    "    for index, row in df.iterrows() :\n",
    "        origin = path + \"dataset-master/JPEGImages/\" + row[\"Image\"]\n",
    "        target = path + folder + \"/\" + row[\"Category\"] + \"/\" + row[\"Image\"]\n",
    "        if(os.path.isfile(origin)) :\n",
    "            shutil.copyfile(origin, target)\n",
    "        else :\n",
    "            print(origin + \" is in the csv, but does not exist\")\n",
    "          \n",
    "def build_training_and_validation(path, df, valid_size) :\n",
    "    # 1 : separate training and validation set\n",
    "    train_df, valid_df = train_test_split(\n",
    "        df, \n",
    "        test_size = valid_size,\n",
    "        stratify = df['Category'])\n",
    "    \n",
    "    # 2 : build training and validation folders\n",
    "    build_folder(path, \"training\", train_df)\n",
    "    build_folder(path, \"validation\", valid_df)\n",
    "\n",
    "##### MAIN ######\n",
    "# path where you want to store your data. Must contain the dataset-master file\n",
    "path = \"./data/\"\n",
    "\n",
    "# how you with to transform the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((120,120)),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "\n",
    "\n",
    "# I : read the labels\n",
    "df = load_csv(path)\n",
    "\n",
    "# II : separate training and validation set\n",
    "build_training_and_validation(path, df, 0.05)\n",
    "\n",
    "# III : build\n",
    "net = BloodCell()\n",
    "batch_size = 128\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-2)\n",
    "\n",
    "training_set = ImageFolder(path+\"training\",transform = transform)\n",
    "validation_set = ImageFolder(path+\"validation\",transform = transform)\n",
    "\n",
    "training_loader = DataLoader(training_set, batch_size, shuffle = True)\n",
    "validation_loader = DataLoader(validation_set, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ffe58643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t Training Loss: 0.9868676861127218 \t\t Validation Loss: 1.1191450357437134\n",
      "Epoch 2 \t Training Loss: 1.0033438603083293 \t\t Validation Loss: 1.0804510116577148\n",
      "Epoch 3 \t Training Loss: 0.9762939016024271 \t\t Validation Loss: 1.0644100904464722\n",
      "Epoch 4 \t Training Loss: 0.9672296841939291 \t\t Validation Loss: 1.1266156435012817\n",
      "Epoch 5 \t Training Loss: 0.986436128616333 \t\t Validation Loss: 1.1443185806274414\n",
      "Epoch 6 \t Training Loss: 1.0142105221748352 \t\t Validation Loss: 1.1317776441574097\n",
      "Epoch 7 \t Training Loss: 1.0070547858874004 \t\t Validation Loss: 1.1558817625045776\n",
      "Epoch 8 \t Training Loss: 0.9805910388628641 \t\t Validation Loss: 1.2118123769760132\n",
      "Epoch 9 \t Training Loss: 1.0219677686691284 \t\t Validation Loss: 1.1996800899505615\n",
      "Epoch 10 \t Training Loss: 0.9493559002876282 \t\t Validation Loss: 1.1676915884017944\n",
      "Epoch 11 \t Training Loss: 0.9970597227414449 \t\t Validation Loss: 1.1284245252609253\n",
      "Epoch 12 \t Training Loss: 0.9715195298194885 \t\t Validation Loss: 1.116477608680725\n",
      "Epoch 13 \t Training Loss: 0.9359712799390157 \t\t Validation Loss: 1.0967987775802612\n",
      "Epoch 14 \t Training Loss: 0.9493685166041056 \t\t Validation Loss: 1.0745779275894165\n",
      "Epoch 15 \t Training Loss: 0.9325792988141378 \t\t Validation Loss: 1.1546199321746826\n",
      "Epoch 16 \t Training Loss: 0.97141961256663 \t\t Validation Loss: 1.1706953048706055\n",
      "Epoch 17 \t Training Loss: 0.9635305603345236 \t\t Validation Loss: 1.1552059650421143\n",
      "Epoch 18 \t Training Loss: 0.9656314253807068 \t\t Validation Loss: 1.2049736976623535\n",
      "Epoch 19 \t Training Loss: 0.950668215751648 \t\t Validation Loss: 1.206372618675232\n",
      "Epoch 20 \t Training Loss: 0.940500537554423 \t\t Validation Loss: 1.155645489692688\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "epochs = 20\n",
    "min_valid_loss = np.inf\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    net.train()\n",
    "    for data, labels in training_loader :\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = criterion(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    #valid_accuracy = 0.0\n",
    "    net.eval()\n",
    "    for data, labels in validation_loader:\n",
    "        output = net(data)\n",
    "        loss = criterion(output,labels)\n",
    "        valid_loss += loss.item()   \n",
    "        #valid_accuracy += 100*(output == labels).float().sum()\n",
    "                \n",
    "                \n",
    "\n",
    "    print(f'Epoch {e+1} \\t Training Loss: {train_loss / len(training_loader)} \\t\\t Validation Loss: {valid_loss / len(validation_loader)}')\n",
    "    #print(f'Epoch {e+1} \\t Training Accuracy: {train_accuracy / len(training_loader)} \\t\\t Validation Accuracy: {valid_accuracy / len(validation_loader)}')\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "93c3734c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52319c84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
